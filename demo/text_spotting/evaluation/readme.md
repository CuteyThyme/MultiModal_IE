## Evaluation toolThis evaluation tools is modified from the official [ICDAR2015 competition](https://rrc.cvc.uab.es/?ch=4). The code is slightly modified to be compatible with python3 and curved text instances.We provide some of the popular benchmarks, including [ICDAR2013](https://rrc.cvc.uab.es/?ch=2), [ICDAR2015](https://rrc.cvc.uab.es/?ch=4), [Total-Text](https://github.com/cs-chan/Total-Text-Dataset), and all of the ground-truthes are transformed into the requried format.The default evaluation metric sets IoU constraint as 0.5.For MANGO which is without accurate text detection branch, The IoU constraint is set as 0.1.#### Do evaluationDirectly run	python script.py -g=gts/gt-icdar2013.zip -s=preds/mango_r50_ic13_none.zip -word_spotting=false -iou=0.1	will produce	num_gt, num_det: 917 1038	Origin:	recall: 0.795 precision: 0.8265 hmean: 0.81Go into the directory of each algorithm for detailed evaluation results.