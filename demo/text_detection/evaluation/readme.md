## Evaluation toolThis evaluation tools is from the repository of [SCUT-CTW1500](https://github.com/Yuliang-Liu/TIoU-metric/tree/master/curved-tiou). The code is slightly modified to be compatibled with python3.We provide some of the popular benchmarks, including [ICDAR2013](https://rrc.cvc.uab.es/?ch=2), [ICDAR2015](https://rrc.cvc.uab.es/?ch=4), [Total-Text](https://github.com/cs-chan/Total-Text-Dataset) and [SCUT-CTW1500](https://github.com/Yuliang-Liu/Curve-Text-Detector), and all of the ground-truthes are transformed into the requried format.The default evaluation metric sets IoU constraint as 0.5.#### Do evaluationDirectly run	python script.py -g=gt/total-text-gt.zip -s=pred/pred_tp_det_r50_tt_e25-45b1f5cf.zip	will produce	num_gt, num_det: 2214 2366	Origin:	recall: 0.8234 precision: 0.8632 hmean: 0.8428Go into the directory of each algorithm for detailed evaluation results.